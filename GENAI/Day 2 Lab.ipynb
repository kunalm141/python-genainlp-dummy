{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6bd9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lab work: With respect to Text provided below: \n",
    "Sample Texts:\n",
    "Mary jumps in a field and following her Sam also jumped. \n",
    "That our lives would be changed forever. The world was loud with carnage and sirens and then quiet with missing voices that would never be heard again.\n",
    "These lives remain precious to our country and infinitely precious to many of you. Today, we remember your loss, we share your sorrow, and we honor the men and women you have loved so long and so well. For those too young to recall that clear September day, it is hard to describe the mix of feelings we experienced.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb33ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Write a Python script that takes a paragraph of text and performs word tokenization using NLTK. Print the list of tokens \n",
    "2. Write a Python script that performs POS tagging on a list of tokens using NLTK. Print the list of tuples containing the word and its POS tag \n",
    "3.Write a Python script that applies stemming to a list of words using NLTK's Porter Stemmer. Print the list of stemmed words \n",
    "4. Write a Python script that applies lemmatization to a list of words using NLTK's WordNet Lemmatize. Print the list of lemmatized words. \n",
    "5.Write a Python script that combines word tokenization, POS tagging, stemming, and lemmatization. Print the results at each step for a given paragraph of text\n",
    " like 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be788f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "012fea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"Mary jumps in a field and following her Sam also jumped,dancing,danced,dancer,dance.\"\n",
    "sent2 = \"That our lives would be changed forever. The world was loud with carnage and sirens and then quiet with missing voices that would never be heard again.\"\n",
    "sent3 = \"These lives remain precious to our country and infinitely precious to many of you. Today, we remember your loss, we share your sorrow, and we honor the men and women you have loved so long and so well. For those too young to recall that clear September day, it is hard to describe the mix of feelings we experienced.\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ab540ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mary', 'jumps', 'in', 'a', 'field', 'and', 'following', 'her', 'Sam', 'also', 'jumped', ',', 'dancing', ',', 'danced', ',', 'dancer', ',', 'dance', '.']\n",
      "\n",
      "\n",
      "['That', 'our', 'lives', 'would', 'be', 'changed', 'forever', '.', 'The', 'world', 'was', 'loud', 'with', 'carnage', 'and', 'sirens', 'and', 'then', 'quiet', 'with', 'missing', 'voices', 'that', 'would', 'never', 'be', 'heard', 'again', '.']\n",
      "\n",
      "\n",
      "['These', 'lives', 'remain', 'precious', 'to', 'our', 'country', 'and', 'infinitely', 'precious', 'to', 'many', 'of', 'you', '.', 'Today', ',', 'we', 'remember', 'your', 'loss', ',', 'we', 'share', 'your', 'sorrow', ',', 'and', 'we', 'honor', 'the', 'men', 'and', 'women', 'you', 'have', 'loved', 'so', 'long', 'and', 'so', 'well', '.', 'For', 'those', 'too', 'young', 'to', 'recall', 'that', 'clear', 'September', 'day', ',', 'it', 'is', 'hard', 'to', 'describe', 'the', 'mix', 'of', 'feelings', 'we', 'experienced', '.']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question1. Write a Python script that takes a paragraph of text and performs word tokenization using NLTK. Print the list of tokens \n",
    "\n",
    "\n",
    "# List of paragraphs to be tokenized\n",
    "paragraphs = [sent1, sent2, sent3]\n",
    "\n",
    "for paragraph in paragraphs:\n",
    "    word_tokenizer(paragraph)\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88a3fa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 2. Write a Python script that performs POS tagging on a list of tokens using NLTK. \n",
    "#Print the list of tuples containing the word and its POS tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee69f4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next step is to tag those words by part of speech:\n",
    "combined_word_list = []\n",
    "\n",
    "for sentence in paragraphs:\n",
    "    \n",
    "    word_list = word_tokenize(sentence)\n",
    "    combined_word_list.extend(word_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98a51a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jumps', 'in']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_word_list[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37b7a78c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mary', 'NNP'),\n",
       " ('jumps', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('field', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('following', 'VBG'),\n",
       " ('her', 'PRP$'),\n",
       " ('Sam', 'NNP'),\n",
       " ('also', 'RB'),\n",
       " ('jumped', 'VBD'),\n",
       " (',', ','),\n",
       " ('dancing', 'VBG'),\n",
       " (',', ','),\n",
       " ('danced', 'VBN'),\n",
       " (',', ','),\n",
       " ('dancer', 'NN'),\n",
       " (',', ','),\n",
       " ('dance', 'NN'),\n",
       " ('.', '.'),\n",
       " ('That', 'IN'),\n",
       " ('our', 'PRP$'),\n",
       " ('lives', 'NNS'),\n",
       " ('would', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('changed', 'VBN'),\n",
       " ('forever', 'RB'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('world', 'NN'),\n",
       " ('was', 'VBD'),\n",
       " ('loud', 'JJ'),\n",
       " ('with', 'IN'),\n",
       " ('carnage', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('sirens', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('then', 'RB'),\n",
       " ('quiet', 'JJ'),\n",
       " ('with', 'IN'),\n",
       " ('missing', 'VBG'),\n",
       " ('voices', 'NNS'),\n",
       " ('that', 'WDT'),\n",
       " ('would', 'MD'),\n",
       " ('never', 'RB'),\n",
       " ('be', 'VB'),\n",
       " ('heard', 'RB'),\n",
       " ('again', 'RB'),\n",
       " ('.', '.'),\n",
       " ('These', 'DT'),\n",
       " ('lives', 'NNS'),\n",
       " ('remain', 'VBP'),\n",
       " ('precious', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('our', 'PRP$'),\n",
       " ('country', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('infinitely', 'RB'),\n",
       " ('precious', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('many', 'JJ'),\n",
       " ('of', 'IN'),\n",
       " ('you', 'PRP'),\n",
       " ('.', '.'),\n",
       " ('Today', 'NN'),\n",
       " (',', ','),\n",
       " ('we', 'PRP'),\n",
       " ('remember', 'VBP'),\n",
       " ('your', 'PRP$'),\n",
       " ('loss', 'NN'),\n",
       " (',', ','),\n",
       " ('we', 'PRP'),\n",
       " ('share', 'NN'),\n",
       " ('your', 'PRP$'),\n",
       " ('sorrow', 'NN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('we', 'PRP'),\n",
       " ('honor', 'VBP'),\n",
       " ('the', 'DT'),\n",
       " ('men', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('women', 'NNS'),\n",
       " ('you', 'PRP'),\n",
       " ('have', 'VBP'),\n",
       " ('loved', 'VBN'),\n",
       " ('so', 'RB'),\n",
       " ('long', 'RB'),\n",
       " ('and', 'CC'),\n",
       " ('so', 'RB'),\n",
       " ('well', 'RB'),\n",
       " ('.', '.'),\n",
       " ('For', 'IN'),\n",
       " ('those', 'DT'),\n",
       " ('too', 'RB'),\n",
       " ('young', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('recall', 'VB'),\n",
       " ('that', 'DT'),\n",
       " ('clear', 'JJ'),\n",
       " ('September', 'NNP'),\n",
       " ('day', 'NN'),\n",
       " (',', ','),\n",
       " ('it', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('hard', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('describe', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('mix', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('feelings', 'NNS'),\n",
       " ('we', 'PRP'),\n",
       " ('experienced', 'VBD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "#nltk.download(\"averaged_perceptron_tagger\")\n",
    "pos_tags = nltk.pos_tag(combined_word_list)\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9092200f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.Write a Python script that applies stemming to a list of words using NLTK's Porter Stemmer. \n",
    "#Print the list of stemmed words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7690d04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary   mari\n",
      "jumps   jump\n",
      "in   in\n",
      "a   a\n",
      "field   field\n",
      "and   and\n",
      "following   follow\n",
      "her   her\n",
      "Sam   sam\n",
      "also   also\n",
      "jumped   jump\n",
      ",   ,\n",
      "dancing   danc\n",
      ",   ,\n",
      "danced   danc\n",
      ",   ,\n",
      "dancer   dancer\n",
      ",   ,\n",
      "dance   danc\n",
      ".   .\n",
      "That   that\n",
      "our   our\n",
      "lives   live\n",
      "would   would\n",
      "be   be\n",
      "changed   chang\n",
      "forever   forev\n",
      ".   .\n",
      "The   the\n",
      "world   world\n",
      "was   wa\n",
      "loud   loud\n",
      "with   with\n",
      "carnage   carnag\n",
      "and   and\n",
      "sirens   siren\n",
      "and   and\n",
      "then   then\n",
      "quiet   quiet\n",
      "with   with\n",
      "missing   miss\n",
      "voices   voic\n",
      "that   that\n",
      "would   would\n",
      "never   never\n",
      "be   be\n",
      "heard   heard\n",
      "again   again\n",
      ".   .\n",
      "These   these\n",
      "lives   live\n",
      "remain   remain\n",
      "precious   preciou\n",
      "to   to\n",
      "our   our\n",
      "country   countri\n",
      "and   and\n",
      "infinitely   infinit\n",
      "precious   preciou\n",
      "to   to\n",
      "many   mani\n",
      "of   of\n",
      "you   you\n",
      ".   .\n",
      "Today   today\n",
      ",   ,\n",
      "we   we\n",
      "remember   rememb\n",
      "your   your\n",
      "loss   loss\n",
      ",   ,\n",
      "we   we\n",
      "share   share\n",
      "your   your\n",
      "sorrow   sorrow\n",
      ",   ,\n",
      "and   and\n",
      "we   we\n",
      "honor   honor\n",
      "the   the\n",
      "men   men\n",
      "and   and\n",
      "women   women\n",
      "you   you\n",
      "have   have\n",
      "loved   love\n",
      "so   so\n",
      "long   long\n",
      "and   and\n",
      "so   so\n",
      "well   well\n",
      ".   .\n",
      "For   for\n",
      "those   those\n",
      "too   too\n",
      "young   young\n",
      "to   to\n",
      "recall   recal\n",
      "that   that\n",
      "clear   clear\n",
      "September   septemb\n",
      "day   day\n",
      ",   ,\n",
      "it   it\n",
      "is   is\n",
      "hard   hard\n",
      "to   to\n",
      "describe   describ\n",
      "the   the\n",
      "mix   mix\n",
      "of   of\n",
      "feelings   feel\n",
      "we   we\n",
      "experienced   experienc\n",
      ".   .\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer() ## defining stemmer\n",
    "for i in combined_word_list:\n",
    "    print(i,\" \",ps.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76654a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Write a Python script that applies lemmatization to a list of words using NLTK's\n",
    "#WordNet Lemmatize. Print the list of lemmatized words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "775e089c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary   Mary\n",
      "jumps   jump\n",
      "in   in\n",
      "a   a\n",
      "field   field\n",
      "and   and\n",
      "following   following\n",
      "her   her\n",
      "Sam   Sam\n",
      "also   also\n",
      "jumped   jumped\n",
      ",   ,\n",
      "dancing   dancing\n",
      ",   ,\n",
      "danced   danced\n",
      ",   ,\n",
      "dancer   dancer\n",
      ",   ,\n",
      "dance   dance\n",
      ".   .\n",
      "That   That\n",
      "our   our\n",
      "lives   life\n",
      "would   would\n",
      "be   be\n",
      "changed   changed\n",
      "forever   forever\n",
      ".   .\n",
      "The   The\n",
      "world   world\n",
      "was   wa\n",
      "loud   loud\n",
      "with   with\n",
      "carnage   carnage\n",
      "and   and\n",
      "sirens   siren\n",
      "and   and\n",
      "then   then\n",
      "quiet   quiet\n",
      "with   with\n",
      "missing   missing\n",
      "voices   voice\n",
      "that   that\n",
      "would   would\n",
      "never   never\n",
      "be   be\n",
      "heard   heard\n",
      "again   again\n",
      ".   .\n",
      "These   These\n",
      "lives   life\n",
      "remain   remain\n",
      "precious   precious\n",
      "to   to\n",
      "our   our\n",
      "country   country\n",
      "and   and\n",
      "infinitely   infinitely\n",
      "precious   precious\n",
      "to   to\n",
      "many   many\n",
      "of   of\n",
      "you   you\n",
      ".   .\n",
      "Today   Today\n",
      ",   ,\n",
      "we   we\n",
      "remember   remember\n",
      "your   your\n",
      "loss   loss\n",
      ",   ,\n",
      "we   we\n",
      "share   share\n",
      "your   your\n",
      "sorrow   sorrow\n",
      ",   ,\n",
      "and   and\n",
      "we   we\n",
      "honor   honor\n",
      "the   the\n",
      "men   men\n",
      "and   and\n",
      "women   woman\n",
      "you   you\n",
      "have   have\n",
      "loved   loved\n",
      "so   so\n",
      "long   long\n",
      "and   and\n",
      "so   so\n",
      "well   well\n",
      ".   .\n",
      "For   For\n",
      "those   those\n",
      "too   too\n",
      "young   young\n",
      "to   to\n",
      "recall   recall\n",
      "that   that\n",
      "clear   clear\n",
      "September   September\n",
      "day   day\n",
      ",   ,\n",
      "it   it\n",
      "is   is\n",
      "hard   hard\n",
      "to   to\n",
      "describe   describe\n",
      "the   the\n",
      "mix   mix\n",
      "of   of\n",
      "feelings   feeling\n",
      "we   we\n",
      "experienced   experienced\n",
      ".   .\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "for word in combined_word_list:\n",
    "    print(word,\" \",lem.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37cfe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.Write a Python script that combines word tokenization, POS tagging, stemming, and lemmatization. \n",
    "#Print the results at each step for a given paragraph of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b83ac27d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mary jumps in a field and following her Sam also jumped,dancing,danced,dancer,dance.',\n",
       " 'That our lives would be changed forever. The world was loud with carnage and sirens and then quiet with missing voices that would never be heard again.',\n",
       " 'These lives remain precious to our country and infinitely precious to many of you. Today, we remember your loss, we share your sorrow, and we honor the men and women you have loved so long and so well. For those too young to recall that clear September day, it is hard to describe the mix of feelings we experienced.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01b98cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is the Word tokenization \n",
      "\n",
      "['Mary', 'jumps', 'in', 'a', 'field', 'and', 'following', 'her', 'Sam', 'also', 'jumped', ',', 'dancing', ',', 'danced', ',', 'dancer', ',', 'dance', '.'] \n",
      "\n",
      "['That', 'our', 'lives', 'would', 'be', 'changed', 'forever', '.', 'The', 'world', 'was', 'loud', 'with', 'carnage', 'and', 'sirens', 'and', 'then', 'quiet', 'with', 'missing', 'voices', 'that', 'would', 'never', 'be', 'heard', 'again', '.'] \n",
      "\n",
      "['These', 'lives', 'remain', 'precious', 'to', 'our', 'country', 'and', 'infinitely', 'precious', 'to', 'many', 'of', 'you', '.', 'Today', ',', 'we', 'remember', 'your', 'loss', ',', 'we', 'share', 'your', 'sorrow', ',', 'and', 'we', 'honor', 'the', 'men', 'and', 'women', 'you', 'have', 'loved', 'so', 'long', 'and', 'so', 'well', '.', 'For', 'those', 'too', 'young', 'to', 'recall', 'that', 'clear', 'September', 'day', ',', 'it', 'is', 'hard', 'to', 'describe', 'the', 'mix', 'of', 'feelings', 'we', 'experienced', '.'] \n",
      "\n",
      "Below is the steming tagging\n",
      "\n",
      "['mari', 'jump', 'in', 'a', 'field', 'and', 'follow', 'her', 'sam', 'also', 'jump', ',', 'danc', ',', 'danc', ',', 'dancer', ',', 'danc', '.'] \n",
      "\n",
      "['that', 'our', 'live', 'would', 'be', 'chang', 'forev', '.', 'the', 'world', 'wa', 'loud', 'with', 'carnag', 'and', 'siren', 'and', 'then', 'quiet', 'with', 'miss', 'voic', 'that', 'would', 'never', 'be', 'heard', 'again', '.'] \n",
      "\n",
      "['these', 'live', 'remain', 'preciou', 'to', 'our', 'countri', 'and', 'infinit', 'preciou', 'to', 'mani', 'of', 'you', '.', 'today', ',', 'we', 'rememb', 'your', 'loss', ',', 'we', 'share', 'your', 'sorrow', ',', 'and', 'we', 'honor', 'the', 'men', 'and', 'women', 'you', 'have', 'love', 'so', 'long', 'and', 'so', 'well', '.', 'for', 'those', 'too', 'young', 'to', 'recal', 'that', 'clear', 'septemb', 'day', ',', 'it', 'is', 'hard', 'to', 'describ', 'the', 'mix', 'of', 'feel', 'we', 'experienc', '.'] \n",
      "\n",
      "Below is the pos tagging \n",
      "\n",
      "[('Mary', 'NNP'), ('jumps', 'NNS'), ('in', 'IN'), ('a', 'DT'), ('field', 'NN'), ('and', 'CC'), ('following', 'VBG'), ('her', 'PRP$'), ('Sam', 'NNP'), ('also', 'RB'), ('jumped', 'VBD'), (',', ','), ('dancing', 'VBG'), (',', ','), ('danced', 'VBN'), (',', ','), ('dancer', 'NN'), (',', ','), ('dance', 'NN'), ('.', '.')] \n",
      "\n",
      "[('That', 'DT'), ('our', 'PRP$'), ('lives', 'NNS'), ('would', 'MD'), ('be', 'VB'), ('changed', 'VBN'), ('forever', 'RB'), ('.', '.'), ('The', 'DT'), ('world', 'NN'), ('was', 'VBD'), ('loud', 'JJ'), ('with', 'IN'), ('carnage', 'NN'), ('and', 'CC'), ('sirens', 'NNS'), ('and', 'CC'), ('then', 'RB'), ('quiet', 'JJ'), ('with', 'IN'), ('missing', 'VBG'), ('voices', 'NNS'), ('that', 'WDT'), ('would', 'MD'), ('never', 'RB'), ('be', 'VB'), ('heard', 'RB'), ('again', 'RB'), ('.', '.')] \n",
      "\n",
      "[('These', 'DT'), ('lives', 'NNS'), ('remain', 'VBP'), ('precious', 'JJ'), ('to', 'TO'), ('our', 'PRP$'), ('country', 'NN'), ('and', 'CC'), ('infinitely', 'RB'), ('precious', 'JJ'), ('to', 'TO'), ('many', 'JJ'), ('of', 'IN'), ('you', 'PRP'), ('.', '.'), ('Today', 'NN'), (',', ','), ('we', 'PRP'), ('remember', 'VBP'), ('your', 'PRP$'), ('loss', 'NN'), (',', ','), ('we', 'PRP'), ('share', 'NN'), ('your', 'PRP$'), ('sorrow', 'NN'), (',', ','), ('and', 'CC'), ('we', 'PRP'), ('honor', 'VBP'), ('the', 'DT'), ('men', 'NNS'), ('and', 'CC'), ('women', 'NNS'), ('you', 'PRP'), ('have', 'VBP'), ('loved', 'VBN'), ('so', 'RB'), ('long', 'RB'), ('and', 'CC'), ('so', 'RB'), ('well', 'RB'), ('.', '.'), ('For', 'IN'), ('those', 'DT'), ('too', 'RB'), ('young', 'JJ'), ('to', 'TO'), ('recall', 'VB'), ('that', 'DT'), ('clear', 'JJ'), ('September', 'NNP'), ('day', 'NN'), (',', ','), ('it', 'PRP'), ('is', 'VBZ'), ('hard', 'JJ'), ('to', 'TO'), ('describe', 'VB'), ('the', 'DT'), ('mix', 'NN'), ('of', 'IN'), ('feelings', 'NNS'), ('we', 'PRP'), ('experienced', 'VBD'), ('.', '.')] \n",
      "\n",
      "Below is the lemmatization \n",
      "\n",
      "['Mary', 'jump', 'in', 'a', 'field', 'and', 'following', 'her', 'Sam', 'also', 'jumped', ',', 'dancing', ',', 'danced', ',', 'dancer', ',', 'dance', '.'] \n",
      "\n",
      "['That', 'our', 'life', 'would', 'be', 'changed', 'forever', '.', 'The', 'world', 'wa', 'loud', 'with', 'carnage', 'and', 'siren', 'and', 'then', 'quiet', 'with', 'missing', 'voice', 'that', 'would', 'never', 'be', 'heard', 'again', '.'] \n",
      "\n",
      "['These', 'life', 'remain', 'precious', 'to', 'our', 'country', 'and', 'infinitely', 'precious', 'to', 'many', 'of', 'you', '.', 'Today', ',', 'we', 'remember', 'your', 'loss', ',', 'we', 'share', 'your', 'sorrow', ',', 'and', 'we', 'honor', 'the', 'men', 'and', 'woman', 'you', 'have', 'loved', 'so', 'long', 'and', 'so', 'well', '.', 'For', 'those', 'too', 'young', 'to', 'recall', 'that', 'clear', 'September', 'day', ',', 'it', 'is', 'hard', 'to', 'describe', 'the', 'mix', 'of', 'feeling', 'we', 'experienced', '.'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#wordtockenization \n",
    "ps = PorterStemmer() ## defining stemmer\n",
    "\n",
    "\n",
    "print(\"Below is the Word tokenization \\n\")\n",
    "for i in paragraphs:\n",
    "    print(word_tokenize(i), \"\\n\")\n",
    "\n",
    "print(\"Below is the steming tagging\\n\") #expects individual words\n",
    "for i in paragraphs:\n",
    "    tokenized_words =  word_tokenize(i)\n",
    "    stemmed_words = [ps.stem(word) for word in tokenized_words]\n",
    "    print(stemmed_words, \"\\n\")\n",
    "    \n",
    "#POS tagging expcects list of words\n",
    "print(\"Below is the pos tagging \\n\")\n",
    "\n",
    "for i in paragraphs:\n",
    "    tokenized_words =  word_tokenize(i)\n",
    "    pos_words = nltk.pos_tag(tokenized_words)\n",
    "    print(pos_words, \"\\n\")\n",
    "\n",
    "print(\"Below is the lemmatization \\n\")\n",
    "for i in paragraphs:\n",
    "    tokenized_words =  word_tokenize(i)\n",
    "    lem_words = [lem.lemmatize(word) for word in tokenized_words] \n",
    "    print(lem_words, \"\\n\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdb9087",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
